name: Deploy APPs on EKS

on:
  workflow_dispatch:
    inputs:
      confirm_deploy:
        description: 'Type "DEPLOY" to confirm  deployment'
        required: true
        type: string

jobs:
  validate:
    name: Validate Code
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: Terraform Init (syntax check only)
      run: terraform init -backend=false

    - name: Terraform Validate
      run: terraform validate

  deploy-rancher:
    name: Deploy Rancher to EKS
    runs-on: ubuntu-latest
    needs: validate
    environment: production
    permissions:
      id-token: write
      contents: read    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.ROLE_ARN }}
        role-session-name: GitHubActions-RancherDeploy
        aws-region: us-east-1
        audience: sts.amazonaws.com

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.6.0

    - name: Initialize Terraform
      run: terraform init

    - name: Verify EKS Cluster Exists
      run: |
        CLUSTER_NAME=$(terraform output -raw cluster_name || echo "rancher-eks-cluster")
        if ! aws eks describe-cluster --name "$CLUSTER_NAME" --region us-east-1 > /dev/null 2>&1; then
          echo "❌ EKS cluster not found. Run 'Deploy EKS Cluster' workflow first."
          exit 1
        fi
        echo "✅ EKS cluster found: $CLUSTER_NAME"

    - name: Generate kubeconfig
      run: |
        aws eks update-kubeconfig --region us-east-1 --name $(terraform output -raw cluster_name) --kubeconfig /tmp/kubeconfig
        test -f /tmp/kubeconfig && echo "kubeconfig ready"

    - name: Export KUBECONFIG env
      run: echo "KUBECONFIG=/tmp/kubeconfig" >> $GITHUB_ENV

    - name: Verify Access Entry exists
      run: |
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        PRINCIPAL_ARN=${{ secrets.ROLE_ARN }}
        echo "Checking Access Entry for $PRINCIPAL_ARN on $CLUSTER_NAME"
        
        if aws eks describe-access-entry --cluster-name "$CLUSTER_NAME" --principal-arn "$PRINCIPAL_ARN" 2>/dev/null; then
          echo "⚠️ Access Entry found but has issues with {{SessionName}} placeholder"
          echo "Deleting Access Entry to force aws-auth ConfigMap usage..."
          aws eks delete-access-entry --cluster-name "$CLUSTER_NAME" --principal-arn "$PRINCIPAL_ARN"
          echo "✅ Access Entry deleted. Cluster will now use aws-auth ConfigMap"
        else
          echo "✅ No Access Entry found. Cluster will use aws-auth ConfigMap"
        fi

    - name: Verify cluster authentication mode
      run: |
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        AUTH_MODE=$(aws eks describe-cluster --name "$CLUSTER_NAME" --query 'cluster.accessConfig.authenticationMode' --output text)
        echo "Current authentication mode: $AUTH_MODE"
        
        if [ "$AUTH_MODE" = "API" ]; then
          echo "⚠️ Cluster is in API mode. Updating to API_AND_CONFIG_MAP..."
          aws eks update-cluster-config --name "$CLUSTER_NAME" --region us-east-1 \
            --access-config authenticationMode=API_AND_CONFIG_MAP
          
          echo "Waiting for cluster update to complete..."
          for i in {1..60}; do
            STATUS=$(aws eks describe-cluster --name "$CLUSTER_NAME" --query 'cluster.status' --output text)
            echo "Cluster status: $STATUS"
            if [ "$STATUS" = "ACTIVE" ]; then
              sleep 10
              break
            fi
            sleep 10
          done
          
          AUTH_MODE=$(aws eks describe-cluster --name "$CLUSTER_NAME" --query 'cluster.accessConfig.authenticationMode' --output text)
          echo "Updated authentication mode: $AUTH_MODE"
          
          echo "Regenerating kubeconfig after auth mode change..."
          rm -f /tmp/kubeconfig
          aws eks update-kubeconfig --region us-east-1 --name "$CLUSTER_NAME" --kubeconfig /tmp/kubeconfig
          echo "✅ kubeconfig regenerated"
        else
          echo "✅ Cluster already in API_AND_CONFIG_MAP mode"
        fi

    - name: Verify aws-auth ConfigMap
      run: |
        echo "Getting current AWS identity..."
        CURRENT_IDENTITY=$(aws sts get-caller-identity --query 'Arn' --output text)
        echo "Current identity ARN: $CURRENT_IDENTITY"
        
        CLUSTER_NAME=$(terraform output -raw cluster_name)
        PIPELINE_ROLE="${{ secrets.ROLE_ARN }}"
        
        echo "Checking if aws-auth ConfigMap exists..."
        if kubectl --kubeconfig /tmp/kubeconfig get cm aws-auth -n kube-system 2>/dev/null; then
          echo "✅ aws-auth exists in cluster"
        else
          echo "⚠️ aws-auth does not exist - Creating Access Entry for pipeline role instead..."
          
          # Create Access Entry for pipeline role using AWS API (doesn't require k8s permissions)
          if ! aws eks describe-access-entry --cluster-name "$CLUSTER_NAME" --principal-arn "$PIPELINE_ROLE" 2>/dev/null; then
            echo "Creating Access Entry for $PIPELINE_ROLE..."
            aws eks create-access-entry \
              --cluster-name "$CLUSTER_NAME" \
              --principal-arn "$PIPELINE_ROLE" \
              --type STANDARD \
              --username github-actions
            
            echo "Associating cluster-admin policy..."
            aws eks associate-access-policy \
              --cluster-name "$CLUSTER_NAME" \
              --principal-arn "$PIPELINE_ROLE" \
              --access-scope type=cluster \
              --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy
            
            echo "✅ Access Entry created successfully"
            sleep 10
          else
            echo "✅ Access Entry already exists"
          fi
        fi
        
        echo ""
        echo "Regenerating kubeconfig..."
        rm -f /tmp/kubeconfig
        aws eks update-kubeconfig --region us-east-1 --name "$CLUSTER_NAME" --kubeconfig /tmp/kubeconfig
        echo "✅ kubeconfig regenerated"

    - name: Create or import aws-auth ConfigMap
      run: |
        # Try to import if it already exists
        if kubectl --kubeconfig /tmp/kubeconfig get cm aws-auth -n kube-system 2>/dev/null; then
          echo "aws-auth exists, importing to Terraform state..."
          terraform import \
            -var-file=terraform.tfvars \
            -var 'deploy_k8s=true' \
            -var 'pipeline_deployer_role_arn=${{ secrets.ROLE_ARN }}' \
            'module.backend.kubernetes_config_map_v1.aws_auth[0]' kube-system/aws-auth || echo "Already in state"
        fi
        
        # Now apply to ensure it matches desired state
        terraform apply -auto-approve \
          -var-file=terraform.tfvars \
          -var 'deploy_k8s=true' \
          -var 'pipeline_deployer_role_arn=${{ secrets.ROLE_ARN }}' \
          -target=module.backend.kubernetes_config_map_v1.aws_auth

    - name: Validate RBAC permissions
      run: |
        echo "Validating RBAC for pipeline role..."
        for i in {1..20}; do
          CAN_NS=$(kubectl --kubeconfig /tmp/kubeconfig auth can-i create namespaces || echo no)
          CAN_CRD=$(kubectl --kubeconfig /tmp/kubeconfig auth can-i create customresourcedefinitions.apiextensions.k8s.io || echo no)
          echo "Attempt $i - namespaces: $CAN_NS, CRDs: $CAN_CRD"
          if [ "$CAN_NS" = "yes" ] && [ "$CAN_CRD" = "yes" ]; then
            echo "✅ RBAC ready"; break
          fi
          sleep 5
        done

    - name: Force Rancher recreation if service is missing
      run: |
        if ! kubectl --kubeconfig /tmp/kubeconfig get svc rancher -n cattle-system 2>/dev/null; then
          echo "⚠️ Rancher service not found, forcing complete recreation..."
          
          echo "Removing Rancher from Terraform state..."
          terraform state rm 'module.backend.helm_release.rancher[0]' || echo "Not in state"
          
          echo "Uninstalling Rancher helm release from cluster..."
          helm uninstall rancher -n cattle-system --kubeconfig /tmp/kubeconfig || echo "Helm release not found"
          
          echo "Waiting for pods to terminate..."
          sleep 10
        fi

    - name: Plan Terraform (K8s/Helm)
      run: |
        terraform plan -refresh=false -compact-warnings \
          -var-file=terraform.tfvars \
          -var 'deploy_k8s=true' \
          -var 'pipeline_deployer_role_arn=${{ secrets.ROLE_ARN }}' \
          -out=tfplan_k8s || true

    - name: Apply Terraform (K8s/Helm)
      run: terraform apply -auto-approve tfplan_k8s

    - name: Restart Rancher if ClusterRoleBinding was created
      run: |
        if kubectl --kubeconfig /tmp/kubeconfig get clusterrolebinding rancher-admin 2>/dev/null; then
          echo "ClusterRoleBinding exists, restarting Rancher to apply new permissions..."
          kubectl --kubeconfig /tmp/kubeconfig rollout restart deployment rancher -n cattle-system
          echo "Waiting for rollout to complete..."
          kubectl --kubeconfig /tmp/kubeconfig rollout status deployment rancher -n cattle-system --timeout=5m
        fi

    - name: Wait for Rancher to be Ready
      run: |
        echo "Waiting for Rancher deployment..."
        for i in {1..30}; do
          READY=$(kubectl --kubeconfig /tmp/kubeconfig get deploy rancher -n cattle-system -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo 0)
          echo "Attempt $i - Ready replicas: $READY"
          if [ "$READY" -gt 0 ] 2>/dev/null; then
            echo "✅ Rancher is ready"; break
          fi
          sleep 10
        done
        
        echo ""
        echo "Checking Rancher pod status..."
        kubectl --kubeconfig /tmp/kubeconfig get pods -n cattle-system
        
        echo ""
        echo "Checking Rancher logs for errors..."
        kubectl --kubeconfig /tmp/kubeconfig logs -n cattle-system deployment/rancher --tail=100 || echo "Could not get logs"

    - name: Get Node IP for NodePort access
      run: |
        echo "Getting node public IP for NodePort access..."
        NODE_IP=$(kubectl --kubeconfig /tmp/kubeconfig get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="ExternalIP")].address}' 2>/dev/null || echo "")
        if [ -z "$NODE_IP" ]; then
          echo "⚠️ No external IP found. Nodes might be in private subnets."
          echo "Getting internal IP instead..."
          NODE_IP=$(kubectl --kubeconfig /tmp/kubeconfig get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
        fi
        echo "Node IP: $NODE_IP"
        echo "NODE_IP=$NODE_IP" >> $GITHUB_ENV

    - name: Export Terraform Outputs
      run: |
        echo "Terraform Outputs:"
        terraform output

    - name: Rancher Deployment Summary
      run: |
        echo "🚀 RANCHER DEPLOYED!"
        echo "======================"
        echo "Cluster Name: $(terraform output -raw cluster_name)"
        echo "Rancher Namespace: $(terraform output -raw rancher_namespace)"
        echo "Rancher Hostname: $(terraform output -raw rancher_hostname)"
        echo ""
        echo "🌐 Access Rancher via NodePort:"
        echo "Node IP: $NODE_IP"
        echo "Rancher URL: http://$NODE_IP:30080"
        echo ""
        echo "Default credentials:"
        echo "  Username: admin"
        echo "  Password: admin"
